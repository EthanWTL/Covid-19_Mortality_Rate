{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f37971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Activation, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0150265",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_X_train = pd.read_csv('Hospitalized Data/partialcon_A_X_train.csv')\n",
    "A_X_test = pd.read_csv('Hospitalized Data/partialcon_A_X_test.csv')\n",
    "A_Y_train = pd.read_csv('Hospitalized Data/partialcon_A_Y_train.csv')\n",
    "A_Y_test = pd.read_csv('Hospitalized Data/partialcon_A_Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f27077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B_X_train = pd.read_csv('Hospitalized Data/partialcon_B_X_train.csv')\n",
    "B_X_test = pd.read_csv('Hospitalized Data/partialcon_B_X_test.csv')\n",
    "B_Y_train = pd.read_csv('Hospitalized Data/partialcon_B_Y_train.csv')\n",
    "B_Y_test = pd.read_csv('Hospitalized Data/partialcon_B_Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d05d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "C_X_train = pd.read_csv('Hospitalized Data/partialcon_C_X_train.csv')\n",
    "C_X_test = pd.read_csv('Hospitalized Data/partialcon_C_X_test.csv')\n",
    "C_Y_train = pd.read_csv('Hospitalized Data/partialcon_C_Y_train.csv')\n",
    "C_Y_test = pd.read_csv('Hospitalized Data/partialcon_C_Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3737240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_X_train = pd.read_csv('Hospitalized Data/partialcon_D_X_train.csv')\n",
    "D_X_test = pd.read_csv('Hospitalized Data/partialcon_D_X_test.csv')\n",
    "D_Y_train = pd.read_csv('Hospitalized Data/partialcon_D_Y_train.csv')\n",
    "D_Y_test = pd.read_csv('Hospitalized Data/partialcon_D_Y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8171119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([A_X_test,B_X_test,C_X_test,D_X_test])\n",
    "y_test = pd.concat([A_Y_test,B_Y_test,C_Y_test,D_Y_test])\n",
    "\n",
    "X_train = pd.concat([A_X_train,B_X_train,C_X_train,D_X_train])\n",
    "y_train = pd.concat([A_Y_train,B_Y_train,C_Y_train,D_Y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fc05e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients():\n",
    "    A_zip = list(zip(A_X_train.values,A_Y_train))\n",
    "    B_zip = list(zip(B_X_train.values,B_Y_train))\n",
    "    C_zip = list(zip(C_X_train.values,C_Y_train))\n",
    "    D_zip = list(zip(D_X_train.values,D_Y_train))\n",
    "    \n",
    "    shards = [A_zip, B_zip, C_zip,D_zip]\n",
    "    client_names = [\"client_1\",\"client_2\",\"client_3\",\"client_4\"]\n",
    "    dic = {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "    return dic\n",
    "\n",
    "\n",
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(20,1)))\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    #logits = model.predict(X_test, batch_size=100)\n",
    "    logits = model.predict(X_test)\n",
    "    length = len(y_test)\n",
    "    Y_test = tf.reshape(Y_test,(length,1))\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), Y_test)\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c94fcc46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'Cast_1' defined at (most recent call last):\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_16296\\1578888054.py\", line 49, in <module>\n      local_model.fit(clients_batched[client], epochs=1, verbose=0)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 676, in update_state\n      y_true = tf.cast(y_true, self._dtype)\nNode: 'Cast_1'\nCast string to float is not supported\n\t [[{{node Cast_1}}]] [Op:__inference_train_function_1222]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m local_model\u001b[38;5;241m.\u001b[39mset_weights(global_weights)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m#fit local model with client's data\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m \u001b[43mlocal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclients_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m#scale the model weights and add to list\u001b[39;00m\n\u001b[0;32m     52\u001b[0m scaling_factor \u001b[38;5;241m=\u001b[39m weight_scalling_factor(clients_batched, client)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\GPU\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'Cast_1' defined at (most recent call last):\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\traitlets\\config\\application.py\", line 1043, in launch_instance\n      app.start()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 383, in do_execute\n      res = shell.run_cell(\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Ethan\\AppData\\Local\\Temp\\ipykernel_16296\\1578888054.py\", line 49, in <module>\n      local_model.fit(clients_batched[client], epochs=1, verbose=0)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1055, in train_step\n      return self.compute_metrics(x, y, y_pred, sample_weight)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\training.py\", line 1149, in compute_metrics\n      self.compiled_metrics.update_state(y, y_pred, sample_weight)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 605, in update_state\n      metric_obj.update_state(y_t, y_p, sample_weight=mask)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\utils\\metrics_utils.py\", line 77, in decorated\n      update_op = update_state_fn(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 140, in update_state_fn\n      return ag_update_state(*args, **kwargs)\n    File \"D:\\Anaconda\\envs\\GPU\\lib\\site-packages\\keras\\metrics\\base_metric.py\", line 676, in update_state\n      y_true = tf.cast(y_true, self._dtype)\nNode: 'Cast_1'\nCast string to float is not supported\n\t [[{{node Cast_1}}]] [Op:__inference_train_function_1222]"
     ]
    }
   ],
   "source": [
    "#create clients\n",
    "clients = create_clients()\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "comms_round = 100\n",
    "    \n",
    "#create optimizer\n",
    "lr = 0.01 \n",
    "loss='sparse_categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "optimizer = tf.keras.optimizers.legacy.SGD(lr=lr, decay=lr / comms_round, momentum=0.9) \n",
    "\n",
    "#initialize global model\n",
    "smlp_global = CNN()\n",
    "global_model = smlp_global.build(20, 2)\n",
    "        \n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = CNN()\n",
    "        local_model = smlp_local.build(20, 2)\n",
    "        local_model.compile(loss=loss, \n",
    "                      optimizer=optimizer, \n",
    "                      metrics=metrics)\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(250)\n",
    "        smlp_SGD = CNN()\n",
    "        SGD_model = smlp_SGD.build(20, 2) \n",
    "\n",
    "        SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# fit the SGD training data to model\n",
    "_ = SGD_model.fit(SGD_dataset, epochs=100, verbose=0)\n",
    "\n",
    "#test the SGD global model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "        SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b13ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_predictions = np.argmax(SGD_model.predict(X_test),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bc551a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 95,  14],\n",
       "       [ 28, 140]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = confusion_matrix(Y_predictions, Y_test)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a83e0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.77      0.82       123\n",
      "           1       0.83      0.91      0.87       154\n",
      "\n",
      "    accuracy                           0.85       277\n",
      "   macro avg       0.85      0.84      0.84       277\n",
      "weighted avg       0.85      0.85      0.85       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56495b0e",
   "metadata": {},
   "source": [
    "# Testing on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec02651a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41  3]\n",
      " [10 37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.93      0.86        44\n",
      "           1       0.93      0.79      0.85        47\n",
      "\n",
      "    accuracy                           0.86        91\n",
      "   macro avg       0.86      0.86      0.86        91\n",
      "weighted avg       0.87      0.86      0.86        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_cle = np.argmax(SGD_model.predict(cle_X_test),axis = 1)\n",
    "cm_cle = confusion_matrix(Y_cle, cle_Y_test_binary)\n",
    "print(cm_cle)\n",
    "print(classification_report(Y_cle, cle_Y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b7cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 19, 28, 32, 38, 46, 53, 67, 69, 70, 77, 90]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_cle, cle_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d4cc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3  4]\n",
      " [10 43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.43      0.30         7\n",
      "           1       0.91      0.81      0.86        53\n",
      "\n",
      "    accuracy                           0.77        60\n",
      "   macro avg       0.57      0.62      0.58        60\n",
      "weighted avg       0.84      0.77      0.79        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_vir = np.argmax(SGD_model.predict(vir_X_test),axis = 1)\n",
    "cm_vir = confusion_matrix(Y_vir, vir_Y_test_binary)\n",
    "print(cm_vir)\n",
    "print(classification_report(Y_vir, vir_Y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "625cfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 6, 11, 14, 22, 32, 34, 40, 43, 48, 49, 56, 57, 58]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_vir, vir_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c031520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50  4]\n",
      " [ 7 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90        54\n",
      "           1       0.88      0.80      0.84        35\n",
      "\n",
      "    accuracy                           0.88        89\n",
      "   macro avg       0.88      0.86      0.87        89\n",
      "weighted avg       0.88      0.88      0.88        89\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_hun = np.argmax(SGD_model.predict(hun_X_test),axis = 1)\n",
    "cm_hun = confusion_matrix(Y_hun, hun_Y_test_binary)\n",
    "print(cm_hun)\n",
    "print(classification_report(Y_hun, hun_Y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec7ffbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 13, 14, 23, 29, 36, 42, 45, 46, 54, 85]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_hun, hun_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e837211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  3]\n",
      " [ 1 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.25      0.33         4\n",
      "           1       0.91      0.97      0.94        33\n",
      "\n",
      "    accuracy                           0.89        37\n",
      "   macro avg       0.71      0.61      0.64        37\n",
      "weighted avg       0.87      0.89      0.88        37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_swi = np.argmax(SGD_model.predict(swi_X_test),axis = 1)\n",
    "cm_swi = confusion_matrix(Y_swi, swi_Y_test_binary)\n",
    "print(cm_swi)\n",
    "print(classification_report(Y_swi, swi_Y_test_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805b6caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 6, 20, 27]\n"
     ]
    }
   ],
   "source": [
    "mismatch = [i for i, (a,b) in enumerate(zip(Y_swi, swi_Y_test_binary)) if a != b]\n",
    "print(mismatch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
