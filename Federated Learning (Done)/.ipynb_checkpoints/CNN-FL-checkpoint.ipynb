{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9f37971",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ethan\\AppData\\Roaming\\Python\\Python39\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Activation, Flatten, Dense, Dropout, SimpleRNN\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05fdffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#binarize the labels\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_files = ['cle_train.csv','cle_test.csv','hun_train.csv','hun_test.csv','swi_train.csv','swi_test.csv','vir_train.csv','vir_test.csv']\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for file in data_files:\n",
    "    data = pd.read_csv('../TrainTestData/' + file)\n",
    "    \n",
    "    X = data.iloc[:, :-1]\n",
    "    Y = data.iloc[:, -1]\n",
    "    \n",
    "    Y_binary = Y.apply(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # Extract the name from the file path\n",
    "    name = file.split('.')[0]\n",
    "    \n",
    "    # Store the dataset components in a dictionary\n",
    "    datasets[name] = {'X': X, 'Y': Y, 'Y_binary': Y_binary}\n",
    "\n",
    "# Unpack the dictionary values in a loop\n",
    "variables = ['cle', 'hun', 'swi', 'vir']\n",
    "train_test = ['train', 'test']\n",
    "\n",
    "for var in variables:\n",
    "    for tt in train_test:\n",
    "        X, Y, Y_binary = datasets[f'{var}_{tt}'].values()\n",
    "        globals()[f'{var}_X_{tt}'] = X\n",
    "        globals()[f'{var}_Y_{tt}'] = Y\n",
    "        globals()[f'{var}_Y_{tt}_binary'] = Y_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d197494f",
   "metadata": {},
   "source": [
    "# creating train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8171119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.concat([cle_X_test,hun_X_test,swi_X_test,vir_X_test])\n",
    "y_test = pd.concat([cle_Y_test_binary,hun_Y_test_binary,swi_Y_test_binary,vir_Y_test_binary])\n",
    "\n",
    "X_train = pd.concat([cle_X_train,hun_X_train,swi_X_train,vir_X_train])\n",
    "y_train = pd.concat([cle_Y_train_binary,hun_Y_train_binary,swi_Y_train_binary,vir_Y_train_binary])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc05e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients():\n",
    "    cle_zip = list(zip(cle_X_train.values,cle_Y_train_binary))\n",
    "    hun_zip = list(zip(hun_X_train.values,hun_Y_train_binary))\n",
    "    vir_zip = list(zip(vir_X_train.values,vir_Y_train_binary))\n",
    "    swi_zip = list(zip(swi_X_train.values,swi_Y_train_binary))\n",
    "    \n",
    "    shards = [cle_zip, hun_zip, vir_zip,swi_zip]\n",
    "    client_names = [\"client_1\",\"client_2\",\"client_3\",\"client_4\"]\n",
    "    dic = {client_names[i] : shards[i] for i in range(len(client_names))}\n",
    "    return dic\n",
    "\n",
    "\n",
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds object'''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)\n",
    "\n",
    "\n",
    "class CNN:\n",
    "    @staticmethod\n",
    "    def build():\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(filters=352, kernel_size=3, activation='relu', input_shape=(35,1)))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Conv1D(filters=384, kernel_size=3, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                      metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "\n",
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count\n",
    "\n",
    "\n",
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final\n",
    "\n",
    "\n",
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad\n",
    "\n",
    "\n",
    "def test_model(X_test, Y_test,  model, comm_round):\n",
    "    cce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    length = len(y_test)\n",
    "    Y_test = tf.reshape(Y_test,(length,1))\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), Y_test)\n",
    "    cm_cle = confusion_matrix(tf.argmax(logits, axis=1), Y_test)\n",
    "    recall = cm_cle[1][1]/(cm_cle[1][1]+cm_cle[0][1])\n",
    "    precision = cm_cle[1][1]/(cm_cle[1][1]+cm_cle[1][0])\n",
    "    print('comm_round: {} | global_acc: {:.3%} | global_loss: {} | recall_1: {:.4%} | precision_1: {:.4%}'\n",
    "          .format(comm_round, acc, loss,recall,precision))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a609618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 0 | global_acc: 73.874% | global_loss: 0.6462017297744751 | recall_1: 39.8970% | precision_1: 87.4805%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 1 | global_acc: 76.149% | global_loss: 0.6098161935806274 | recall_1: 56.0449% | precision_1: 77.6561%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 2 | global_acc: 76.144% | global_loss: 0.6098890900611877 | recall_1: 52.9814% | precision_1: 80.2057%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 3 | global_acc: 76.236% | global_loss: 0.6039765477180481 | recall_1: 54.9083% | precision_1: 78.8008%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 4 | global_acc: 76.244% | global_loss: 0.6056383848190308 | recall_1: 59.5835% | precision_1: 75.4144%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 5 | global_acc: 76.280% | global_loss: 0.6043745875358582 | recall_1: 56.5600% | precision_1: 77.6153%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 6 | global_acc: 76.177% | global_loss: 0.610741376876831 | recall_1: 60.3960% | precision_1: 74.7459%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 7 | global_acc: 76.223% | global_loss: 0.6063651442527771 | recall_1: 59.4903% | precision_1: 75.4236%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 8 | global_acc: 76.341% | global_loss: 0.602868914604187 | recall_1: 57.2215% | precision_1: 77.2847%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 9 | global_acc: 76.304% | global_loss: 0.6025665998458862 | recall_1: 58.2782% | precision_1: 76.4339%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 10 | global_acc: 76.237% | global_loss: 0.603376567363739 | recall_1: 59.4237% | precision_1: 75.5006%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 11 | global_acc: 76.318% | global_loss: 0.6022725701332092 | recall_1: 57.9807% | precision_1: 76.6778%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 12 | global_acc: 76.384% | global_loss: 0.6023432612419128 | recall_1: 57.1194% | precision_1: 77.4720%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 13 | global_acc: 76.338% | global_loss: 0.6045035719871521 | recall_1: 59.0197% | precision_1: 76.0121%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 14 | global_acc: 76.297% | global_loss: 0.6041219234466553 | recall_1: 57.7321% | precision_1: 76.8000%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 15 | global_acc: 76.408% | global_loss: 0.6027822494506836 | recall_1: 58.6112% | precision_1: 76.4655%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 16 | global_acc: 76.451% | global_loss: 0.602210521697998 | recall_1: 57.9097% | precision_1: 77.0681%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 17 | global_acc: 76.405% | global_loss: 0.6023406386375427 | recall_1: 55.7341% | precision_1: 78.6036%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 18 | global_acc: 76.311% | global_loss: 0.601409375667572 | recall_1: 60.1296% | precision_1: 75.2264%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 19 | global_acc: 76.407% | global_loss: 0.6010299324989319 | recall_1: 55.4411% | precision_1: 78.8470%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 20 | global_acc: 76.410% | global_loss: 0.6008297204971313 | recall_1: 57.1593% | precision_1: 77.5122%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 21 | global_acc: 76.387% | global_loss: 0.6049098372459412 | recall_1: 56.7820% | precision_1: 77.7352%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 22 | global_acc: 76.422% | global_loss: 0.5988553166389465 | recall_1: 57.3236% | precision_1: 77.4226%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 23 | global_acc: 76.288% | global_loss: 0.603334367275238 | recall_1: 59.8633% | precision_1: 75.3409%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 24 | global_acc: 76.341% | global_loss: 0.6000044345855713 | recall_1: 56.6887% | precision_1: 77.6831%\n",
      "1773/1773 [==============================] - 2s 993us/step\n",
      "comm_round: 25 | global_acc: 76.362% | global_loss: 0.6040509939193726 | recall_1: 57.7277% | precision_1: 76.9713%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 26 | global_acc: 76.405% | global_loss: 0.6000797748565674 | recall_1: 57.4213% | precision_1: 77.3042%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 27 | global_acc: 76.410% | global_loss: 0.6019273996353149 | recall_1: 57.1682% | precision_1: 77.5056%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 28 | global_acc: 76.348% | global_loss: 0.6010130047798157 | recall_1: 59.1884% | precision_1: 75.9255%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 29 | global_acc: 76.340% | global_loss: 0.599882960319519 | recall_1: 57.7099% | precision_1: 76.9249%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 30 | global_acc: 76.355% | global_loss: 0.5999481081962585 | recall_1: 57.9319% | precision_1: 76.8072%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 31 | global_acc: 76.396% | global_loss: 0.6009647250175476 | recall_1: 58.0207% | precision_1: 76.8480%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 32 | global_acc: 76.384% | global_loss: 0.6008748412132263 | recall_1: 58.0118% | precision_1: 76.8227%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 33 | global_acc: 76.362% | global_loss: 0.6036120057106018 | recall_1: 58.8865% | precision_1: 76.1629%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 34 | global_acc: 76.421% | global_loss: 0.5998749136924744 | recall_1: 57.5989% | precision_1: 77.2156%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 35 | global_acc: 76.380% | global_loss: 0.5999802947044373 | recall_1: 58.6023% | precision_1: 76.4008%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 36 | global_acc: 76.384% | global_loss: 0.6025671362876892 | recall_1: 56.0760% | precision_1: 78.2722%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 37 | global_acc: 76.385% | global_loss: 0.5992698073387146 | recall_1: 57.3369% | precision_1: 77.3155%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 38 | global_acc: 76.343% | global_loss: 0.601244330406189 | recall_1: 58.9708% | precision_1: 76.0580%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 39 | global_acc: 76.373% | global_loss: 0.5991369485855103 | recall_1: 58.4514% | precision_1: 76.4873%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 40 | global_acc: 76.359% | global_loss: 0.6007214188575745 | recall_1: 57.4701% | precision_1: 77.1486%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 41 | global_acc: 76.361% | global_loss: 0.6031365394592285 | recall_1: 57.1993% | precision_1: 77.3521%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 42 | global_acc: 76.341% | global_loss: 0.6014862060546875 | recall_1: 58.2782% | precision_1: 76.5275%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 43 | global_acc: 76.357% | global_loss: 0.5968775153160095 | recall_1: 57.9896% | precision_1: 76.7707%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 44 | global_acc: 76.422% | global_loss: 0.601693332195282 | recall_1: 57.2748% | precision_1: 77.4589%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 45 | global_acc: 76.407% | global_loss: 0.5978361964225769 | recall_1: 58.0207% | precision_1: 76.8751%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 46 | global_acc: 76.385% | global_loss: 0.5976516604423523 | recall_1: 58.7266% | precision_1: 76.3287%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 47 | global_acc: 76.324% | global_loss: 0.6002032160758972 | recall_1: 59.6457% | precision_1: 75.5653%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 48 | global_acc: 76.396% | global_loss: 0.6024066209793091 | recall_1: 56.1293% | precision_1: 78.2641%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 49 | global_acc: 76.347% | global_loss: 0.5982978343963623 | recall_1: 57.8520% | precision_1: 76.8414%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 50 | global_acc: 76.350% | global_loss: 0.598778486251831 | recall_1: 58.5801% | precision_1: 76.3409%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 51 | global_acc: 76.327% | global_loss: 0.5999576449394226 | recall_1: 57.9230% | precision_1: 76.7412%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 52 | global_acc: 76.347% | global_loss: 0.6020463705062866 | recall_1: 56.9462% | precision_1: 77.5032%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 53 | global_acc: 76.347% | global_loss: 0.5964124798774719 | recall_1: 56.5644% | precision_1: 77.7920%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 54 | global_acc: 76.371% | global_loss: 0.5995737314224243 | recall_1: 56.3779% | precision_1: 78.0023%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 55 | global_acc: 76.329% | global_loss: 0.5979399085044861 | recall_1: 57.7587% | precision_1: 76.8626%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 56 | global_acc: 76.331% | global_loss: 0.6003773808479309 | recall_1: 57.6655% | precision_1: 76.9340%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 57 | global_acc: 76.338% | global_loss: 0.5993435978889465 | recall_1: 57.2037% | precision_1: 77.2885%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 58 | global_acc: 76.262% | global_loss: 0.6012722253799438 | recall_1: 59.1529% | precision_1: 75.7376%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 59 | global_acc: 76.373% | global_loss: 0.5986638069152832 | recall_1: 57.2037% | precision_1: 77.3814%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 60 | global_acc: 76.239% | global_loss: 0.6005944609642029 | recall_1: 59.5347% | precision_1: 75.4332%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 61 | global_acc: 76.315% | global_loss: 0.6007258892059326 | recall_1: 57.7010% | precision_1: 76.8676%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 62 | global_acc: 76.308% | global_loss: 0.6012488007545471 | recall_1: 57.9941% | precision_1: 76.6414%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 63 | global_acc: 76.311% | global_loss: 0.6018585562705994 | recall_1: 57.3680% | precision_1: 77.0989%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 64 | global_acc: 76.297% | global_loss: 0.599610447883606 | recall_1: 59.8810% | precision_1: 75.3506%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 65 | global_acc: 76.306% | global_loss: 0.5989320278167725 | recall_1: 58.1938% | precision_1: 76.4970%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 66 | global_acc: 76.331% | global_loss: 0.5992559194564819 | recall_1: 57.6167% | precision_1: 76.9692%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 67 | global_acc: 76.274% | global_loss: 0.5992594957351685 | recall_1: 57.6699% | precision_1: 76.7853%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 68 | global_acc: 76.308% | global_loss: 0.5995520949363708 | recall_1: 59.0552% | precision_1: 75.9146%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 69 | global_acc: 76.297% | global_loss: 0.5975529551506042 | recall_1: 57.7543% | precision_1: 76.7841%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 70 | global_acc: 76.334% | global_loss: 0.5994510054588318 | recall_1: 57.8475% | precision_1: 76.8129%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 71 | global_acc: 76.322% | global_loss: 0.5996604561805725 | recall_1: 58.4158% | precision_1: 76.3832%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 72 | global_acc: 76.299% | global_loss: 0.6010674834251404 | recall_1: 57.3236% | precision_1: 77.0990%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 73 | global_acc: 76.271% | global_loss: 0.5979093313217163 | recall_1: 57.1638% | precision_1: 77.1420%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 74 | global_acc: 76.258% | global_loss: 0.5995165705680847 | recall_1: 59.0552% | precision_1: 75.7935%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 75 | global_acc: 76.385% | global_loss: 0.5991036295890808 | recall_1: 57.0661% | precision_1: 77.5164%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 76 | global_acc: 76.334% | global_loss: 0.6012608408927917 | recall_1: 57.5323% | precision_1: 77.0392%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 77 | global_acc: 76.338% | global_loss: 0.5969991683959961 | recall_1: 57.3991% | precision_1: 77.1452%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 78 | global_acc: 76.273% | global_loss: 0.598482072353363 | recall_1: 56.3158% | precision_1: 77.7825%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 79 | global_acc: 76.278% | global_loss: 0.5994547605514526 | recall_1: 58.7311% | precision_1: 76.0580%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 80 | global_acc: 76.301% | global_loss: 0.5995309948921204 | recall_1: 57.7898% | precision_1: 76.7679%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 81 | global_acc: 76.288% | global_loss: 0.5985704064369202 | recall_1: 57.7454% | precision_1: 76.7678%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 82 | global_acc: 76.324% | global_loss: 0.5999786257743835 | recall_1: 57.7321% | precision_1: 76.8681%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 83 | global_acc: 76.253% | global_loss: 0.5989988446235657 | recall_1: 58.9664% | precision_1: 75.8394%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 84 | global_acc: 76.294% | global_loss: 0.5991717576980591 | recall_1: 57.7543% | precision_1: 76.7751%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 85 | global_acc: 76.246% | global_loss: 0.6002026796340942 | recall_1: 57.7898% | precision_1: 76.6278%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 86 | global_acc: 76.281% | global_loss: 0.6005035638809204 | recall_1: 56.6576% | precision_1: 77.5462%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 87 | global_acc: 76.292% | global_loss: 0.599095344543457 | recall_1: 56.6044% | precision_1: 77.6148%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 88 | global_acc: 76.290% | global_loss: 0.5987417101860046 | recall_1: 58.0207% | precision_1: 76.5778%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 89 | global_acc: 76.317% | global_loss: 0.599173903465271 | recall_1: 56.8352% | precision_1: 77.5067%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 90 | global_acc: 76.313% | global_loss: 0.5988205075263977 | recall_1: 57.6033% | precision_1: 76.9331%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 91 | global_acc: 76.246% | global_loss: 0.5998887419700623 | recall_1: 56.9995% | precision_1: 77.1978%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 92 | global_acc: 76.241% | global_loss: 0.600731372833252 | recall_1: 57.7143% | precision_1: 76.6676%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 93 | global_acc: 76.220% | global_loss: 0.5995791554450989 | recall_1: 58.2871% | precision_1: 76.2148%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 94 | global_acc: 76.278% | global_loss: 0.5985409021377563 | recall_1: 57.9807% | precision_1: 76.5744%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 95 | global_acc: 76.214% | global_loss: 0.6016411781311035 | recall_1: 58.4647% | precision_1: 76.0804%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 96 | global_acc: 76.271% | global_loss: 0.5997875332832336 | recall_1: 57.3192% | precision_1: 77.0286%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 97 | global_acc: 76.255% | global_loss: 0.5999214053153992 | recall_1: 57.5634% | precision_1: 76.8114%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 98 | global_acc: 76.253% | global_loss: 0.601354718208313 | recall_1: 57.7188% | precision_1: 76.6962%\n",
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "comm_round: 99 | global_acc: 76.271% | global_loss: 0.599419116973877 | recall_1: 58.1406% | precision_1: 76.4448%\n"
     ]
    }
   ],
   "source": [
    "# create clients\n",
    "clients = create_clients()\n",
    "\n",
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "comms_round = 100\n",
    "\n",
    "#initialize global model\n",
    "smlp_global = CNN()\n",
    "global_model = smlp_global.build()\n",
    "\n",
    "losslist = []\n",
    "#commence global training loop\n",
    "for comm_round in range(comms_round):\n",
    "            \n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        smlp_local = CNN()\n",
    "        local_model = smlp_local.build()\n",
    "        local_model.compile(loss='sparse_categorical_crossentropy', \n",
    "                      optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "                      metrics=['accuracy'])\n",
    "        \n",
    "        #set local model weight to the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        with tf.device('/device:GPU:0'):\n",
    "            local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n",
    "        losslist.append(global_loss)\n",
    "\n",
    "    global_model.save('../Models/FL_CNN/train/FL_CNN_epoch'+str(comm_round)+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56495b0e",
   "metadata": {},
   "source": [
    "# Testing on each dataset on the best performanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d6a2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_model = tf.keras.models.load_model('../Models/FL_CNN/train/FL_CNN_epoch6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b13ff1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1773/1773 [==============================] - 2s 1ms/step\n",
      "[[29617  8920]\n",
      " [ 4596 13603]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7685    0.8657    0.8142     34213\n",
      "           1     0.7475    0.6040    0.6681     22523\n",
      "\n",
      "    accuracy                         0.7618     56736\n",
      "   macro avg     0.7580    0.7348    0.7412     56736\n",
      "weighted avg     0.7602    0.7618    0.7562     56736\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_predictions = np.argmax(global_model.predict(X_test),axis = 1)\n",
    "cm = confusion_matrix(Y_predictions, Y_test)\n",
    "print(cm)\n",
    "print(classification_report(Y_test, Y_predictions, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec02651a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "[8535 5649]\n",
      "[[7386 2235]\n",
      " [1149 3414]]\n",
      "actual recall for class 1 is: 0.6043547530536378\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8654    0.7677    0.8136      9621\n",
      "           1     0.6044    0.7482    0.6686      4563\n",
      "\n",
      "    accuracy                         0.7614     14184\n",
      "   macro avg     0.7349    0.7579    0.7411     14184\n",
      "weighted avg     0.7814    0.7614    0.7670     14184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_cle = np.argmax(global_model.predict(cle_X_test),axis = 1)\n",
    "cm_cle = confusion_matrix(Y_cle, cle_Y_test_binary)\n",
    "print(np.bincount(cle_Y_test_binary))\n",
    "print(cm_cle)\n",
    "recall = cm_cle[1][1]/np.bincount(cle_Y_test_binary)[1]\n",
    "print('actual recall for class 1 is: ' + str(recall))\n",
    "print(classification_report(Y_cle, cle_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2d4cc1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 2ms/step\n",
      "[8647 5537]\n",
      "[[7480 2181]\n",
      " [1167 3356]]\n",
      "actual recall for class 1 is: 0.6061043886581181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8650    0.7742    0.8171      9661\n",
      "           1     0.6061    0.7420    0.6672      4523\n",
      "\n",
      "    accuracy                         0.7640     14184\n",
      "   macro avg     0.7356    0.7581    0.7422     14184\n",
      "weighted avg     0.7825    0.7640    0.7693     14184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_vir = np.argmax(global_model.predict(vir_X_test),axis = 1)\n",
    "cm_vir = confusion_matrix(Y_vir, vir_Y_test_binary)\n",
    "print(np.bincount(vir_Y_test_binary))\n",
    "print(cm_vir)\n",
    "recall = cm_vir[1][1]/np.bincount(vir_Y_test_binary)[1]\n",
    "print('actual recall for class 1 is: ' + str(recall))\n",
    "print(classification_report(Y_vir, vir_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c031520",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 1ms/step\n",
      "[8541 5643]\n",
      "[[7414 2238]\n",
      " [1127 3405]]\n",
      "actual recall for class 1 is: 0.6034024455077087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8680    0.7681    0.8150      9652\n",
      "           1     0.6034    0.7513    0.6693      4532\n",
      "\n",
      "    accuracy                         0.7628     14184\n",
      "   macro avg     0.7357    0.7597    0.7422     14184\n",
      "weighted avg     0.7835    0.7628    0.7685     14184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_hun = np.argmax(global_model.predict(hun_X_test),axis = 1)\n",
    "cm_hun = confusion_matrix(Y_hun, hun_Y_test_binary)\n",
    "print(np.bincount(hun_Y_test_binary))\n",
    "print(cm_hun)\n",
    "recall = cm_hun[1][1]/np.bincount(hun_Y_test_binary)[1]\n",
    "print('actual recall for class 1 is: ' + str(recall))\n",
    "print(classification_report(Y_hun, hun_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e837211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "444/444 [==============================] - 1s 1ms/step\n",
      "[8490 5694]\n",
      "[[7337 2266]\n",
      " [1153 3428]]\n",
      "actual recall for class 1 is: 0.6020372321742185\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8642    0.7640    0.8110      9603\n",
      "           1     0.6020    0.7483    0.6673      4581\n",
      "\n",
      "    accuracy                         0.7590     14184\n",
      "   macro avg     0.7331    0.7562    0.7391     14184\n",
      "weighted avg     0.7795    0.7590    0.7646     14184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_swi = np.argmax(global_model.predict(swi_X_test),axis = 1)\n",
    "cm_swi = confusion_matrix(Y_swi, swi_Y_test_binary)\n",
    "print(np.bincount(swi_Y_test_binary))\n",
    "print(cm_swi)\n",
    "recall = cm_swi[1][1]/np.bincount(swi_Y_test_binary)[1]\n",
    "print('actual recall for class 1 is: ' + str(recall))\n",
    "print(classification_report(Y_swi, swi_Y_test_binary, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd924b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
